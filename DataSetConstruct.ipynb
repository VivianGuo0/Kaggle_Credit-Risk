{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-13T13:25:53.050741Z","iopub.status.busy":"2024-05-13T13:25:53.050159Z","iopub.status.idle":"2024-05-13T13:25:53.606255Z","shell.execute_reply":"2024-05-13T13:25:53.605007Z","shell.execute_reply.started":"2024-05-13T13:25:53.050699Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/feature_definitions.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_deposit_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_cb_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_0_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_3.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_tax_registry_b_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_0_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_3.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_9.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_debitcard_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_11.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_1_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_4.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_tax_registry_c_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_1_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_tax_registry_a_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_6.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_5.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_b_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_other_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_static_0_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_applprev_1_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_base.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_person_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_7.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_10.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_b_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_8.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_1_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_credit_bureau_a_2_4.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test/test_person_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_tax_registry_c_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_static_0_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_3.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_b_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_applprev_1_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_static_cb_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_other_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_6.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_tax_registry_a_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_tax_registry_b_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_person_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_person_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_b_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_7.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_deposit_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_debitcard_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_5.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_4.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_base.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_9.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_3.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_applprev_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_10.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_2_8.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_2.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_static_0_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_credit_bureau_a_1_1.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train/train_applprev_1_0.parquet\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_6.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_11.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_9.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_base.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_b_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_10.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_4.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_0_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_cb_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_b_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_tax_registry_b_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_person_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_person_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_8.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_1_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_1_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_7.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_other_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_tax_registry_c_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_3.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_0_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_5.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_debitcard_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_1_4.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_deposit_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_static_0_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_applprev_1_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_tax_registry_a_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/test/test_credit_bureau_a_2_3.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_3.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_static_cb_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_person_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_base.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_tax_registry_a_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_static_0_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_6.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_person_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_1_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_tax_registry_c_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_4.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_9.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_3.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_7.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_b_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_2.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_static_0_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_deposit_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_10.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_tax_registry_b_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_8.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_5.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_b_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_credit_bureau_a_2_0.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_other_1.csv\n","/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train/train_debitcard_1.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:53.609297Z","iopub.status.busy":"2024-05-13T13:25:53.608773Z","iopub.status.idle":"2024-05-13T13:25:54.517519Z","shell.execute_reply":"2024-05-13T13:25:54.516288Z","shell.execute_reply.started":"2024-05-13T13:25:53.609261Z"},"trusted":true},"outputs":[],"source":["import sys\n","from pathlib import Path\n","import subprocess\n","import os\n","import gc\n","from glob import glob\n","import pickle\n","\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","from datetime import datetime\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:54.519551Z","iopub.status.busy":"2024-05-13T13:25:54.519080Z","iopub.status.idle":"2024-05-13T13:25:56.549575Z","shell.execute_reply":"2024-05-13T13:25:56.548390Z","shell.execute_reply.started":"2024-05-13T13:25:54.519509Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.metrics import roc_auc_score\n","import lightgbm as lgb\n","\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import EditedNearestNeighbours\n","from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n","# from woe_conversion.woe import *\n","from sklearn.impute import KNNImputer\n","\n","from imblearn.combine import SMOTEENN\n","from collections import Counter\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.551758Z","iopub.status.busy":"2024-05-13T13:25:56.551033Z","iopub.status.idle":"2024-05-13T13:25:56.702427Z","shell.execute_reply":"2024-05-13T13:25:56.701256Z","shell.execute_reply.started":"2024-05-13T13:25:56.551714Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.707295Z","iopub.status.busy":"2024-05-13T13:25:56.706410Z","iopub.status.idle":"2024-05-13T13:25:56.714008Z","shell.execute_reply":"2024-05-13T13:25:56.712760Z","shell.execute_reply.started":"2024-05-13T13:25:56.707260Z"},"trusted":true},"outputs":[],"source":["ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n","TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n","TRAIN_CSV_DIR   = ROOT / \"csv_files\" / \"train\"\n","TEST_DIR        = ROOT / \"parquet_files\" / \"test\""]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.716027Z","iopub.status.busy":"2024-05-13T13:25:56.715554Z","iopub.status.idle":"2024-05-13T13:25:56.729448Z","shell.execute_reply":"2024-05-13T13:25:56.728242Z","shell.execute_reply.started":"2024-05-13T13:25:56.715965Z"},"trusted":true},"outputs":[],"source":["import threading\n","\n","def process_files(filename, dir, feature_sheet_list):\n","    '''\n","    处理单个csv\n","    '''\n","    df = pd.read_parquet(os.path.join(dir, filename))\n","    print('Processing: ', filename)\n","\n","    info = pd.DataFrame(df.isnull().sum(), columns=['nan_num'])\n","    info['df_len'] = df.shape[0]\n","    info['file'] = filename\n","    info = info.rename(index={'case_id': f'case_id_{filename[:-4]}'})\n","    \n","    feature_sheet_list.append(info)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.731697Z","iopub.status.busy":"2024-05-13T13:25:56.731138Z","iopub.status.idle":"2024-05-13T13:25:56.745781Z","shell.execute_reply":"2024-05-13T13:25:56.744294Z","shell.execute_reply.started":"2024-05-13T13:25:56.731652Z"},"trusted":true},"outputs":[],"source":["def feature_sheet_process(file_list, dir):\n","    '''\n","    多线程处理\n","    '''\n","    threads = []\n","    feature_sheet_list = []\n","\n","    for file in file_list:\n","        thread = threading.Thread(target=process_files, args=(file, dir, feature_sheet_list))\n","        thread.start()\n","        threads.append(thread)\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    combined_feature_sheet = pd.concat(feature_sheet_list, axis=0)\n","    return combined_feature_sheet"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.747956Z","iopub.status.busy":"2024-05-13T13:25:56.747536Z","iopub.status.idle":"2024-05-13T13:25:56.761336Z","shell.execute_reply":"2024-05-13T13:25:56.760068Z","shell.execute_reply.started":"2024-05-13T13:25:56.747923Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\ndata_store = {}\\nfor i in range(0, 32, 8):\\n    train_feature_sheet = feature_sheet_process(train_list[i:i+8], TRAIN_DIR)\\n    data_store.update({f'feature_sheet_{i//8}': train_feature_sheet})\\n\""]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","data_store = {}\n","for i in range(0, 32, 8):\n","    train_feature_sheet = feature_sheet_process(train_list[i:i+8], TRAIN_DIR)\n","    data_store.update({f'feature_sheet_{i//8}': train_feature_sheet})\n","'''"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.763080Z","iopub.status.busy":"2024-05-13T13:25:56.762676Z","iopub.status.idle":"2024-05-13T13:25:56.775589Z","shell.execute_reply":"2024-05-13T13:25:56.774446Z","shell.execute_reply.started":"2024-05-13T13:25:56.763033Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\ntrain_feature_sheet = data_store['feature_sheet_0.csv']\\nfor i in range(1,4):\\n    train_feature_sheet = pd.concat([train_feature_sheet, data_store[f'feature_sheet_{i}.csv']], axis=0)\\ndel data_store\\n\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","train_feature_sheet = data_store['feature_sheet_0.csv']\n","for i in range(1,4):\n","    train_feature_sheet = pd.concat([train_feature_sheet, data_store[f'feature_sheet_{i}.csv']], axis=0)\n","del data_store\n","'''"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.777420Z","iopub.status.busy":"2024-05-13T13:25:56.777015Z","iopub.status.idle":"2024-05-13T13:25:56.788466Z","shell.execute_reply":"2024-05-13T13:25:56.787250Z","shell.execute_reply.started":"2024-05-13T13:25:56.777374Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\ntrain_feature_sheet = train_feature_sheet.rename(columns={'Unnamed: 0': 'feature'})\\ntrain_feature_sheet\\n\""]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","train_feature_sheet = train_feature_sheet.rename(columns={'Unnamed: 0': 'feature'})\n","train_feature_sheet\n","'''"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.790467Z","iopub.status.busy":"2024-05-13T13:25:56.789970Z","iopub.status.idle":"2024-05-13T13:25:56.804899Z","shell.execute_reply":"2024-05-13T13:25:56.803591Z","shell.execute_reply.started":"2024-05-13T13:25:56.790434Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\ntrain_feature_sheet = train_feature_sheet.groupby('feature').agg({'nan_num': 'sum', 'df_len': 'sum', 'file': list}).reset_index()\\ntrain_feature_sheet['nan_ratio'] = train_feature_sheet['nan_num'] / train_feature_sheet['df_len']\\ntrain_feature_sheet\\n\""]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","train_feature_sheet = train_feature_sheet.groupby('feature').agg({'nan_num': 'sum', 'df_len': 'sum', 'file': list}).reset_index()\n","train_feature_sheet['nan_ratio'] = train_feature_sheet['nan_num'] / train_feature_sheet['df_len']\n","train_feature_sheet\n","'''"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.806979Z","iopub.status.busy":"2024-05-13T13:25:56.806513Z","iopub.status.idle":"2024-05-13T13:25:56.818987Z","shell.execute_reply":"2024-05-13T13:25:56.817416Z","shell.execute_reply.started":"2024-05-13T13:25:56.806935Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\n# 按照feature首字母排序\\ntrain_feature_sheet = train_feature_sheet.sort_values(by='feature')\\ntrain_feature_sheet\\n\""]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","# 按照feature首字母排序\n","train_feature_sheet = train_feature_sheet.sort_values(by='feature')\n","train_feature_sheet\n","'''"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.820972Z","iopub.status.busy":"2024-05-13T13:25:56.820578Z","iopub.status.idle":"2024-05-13T13:25:56.833489Z","shell.execute_reply":"2024-05-13T13:25:56.832356Z","shell.execute_reply.started":"2024-05-13T13:25:56.820941Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\n# train_feature_sheet丢弃feature为'case_id'开头的行\\ntrain_feature_sheet_1 = train_feature_sheet[~train_feature_sheet['feature'].str.startswith('case_id')]\\ndel train_feature_sheet\\ntrain_feature_sheet_1\\n\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","# train_feature_sheet丢弃feature为'case_id'开头的行\n","train_feature_sheet_1 = train_feature_sheet[~train_feature_sheet['feature'].str.startswith('case_id')]\n","del train_feature_sheet\n","train_feature_sheet_1\n","'''"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.839503Z","iopub.status.busy":"2024-05-13T13:25:56.838736Z","iopub.status.idle":"2024-05-13T13:25:56.854318Z","shell.execute_reply":"2024-05-13T13:25:56.853046Z","shell.execute_reply.started":"2024-05-13T13:25:56.839459Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"\\n# feature_definitions里面的feature\\nfeature_list = pd.read_csv(os.path.joint(ROOT, 'feature_definitions.csv'))\\nfeatures1 = feature_list['Variable'].values\\nfeatures0 = train_feature_sheet_1['feature'].values\\nfeatures = list(set(features0) - set(features1))\\nfeatures  # 在csv不在definitions里面的feature(就是train_base里面的)\\n\""]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","# feature_definitions里面的feature\n","feature_list = pd.read_csv(os.path.joint(ROOT, 'feature_definitions.csv'))\n","features1 = feature_list['Variable'].values\n","features0 = train_feature_sheet_1['feature'].values\n","features = list(set(features0) - set(features1))\n","features  # 在csv不在definitions里面的feature(就是train_base里面的)\n","'''"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.856061Z","iopub.status.busy":"2024-05-13T13:25:56.855689Z","iopub.status.idle":"2024-05-13T13:25:56.868250Z","shell.execute_reply":"2024-05-13T13:25:56.867065Z","shell.execute_reply.started":"2024-05-13T13:25:56.856025Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\n# 保留feature_list中Variable列的值在train_feature_sheet_1中的行\\ntrain_feature_sheet_2 = train_feature_sheet_1[train_feature_sheet_1[\\'feature\\'].isin(features1)]\\nfeature_info = pl.DataFrame(train_feature_sheet_2.reset_index())\\ndel train_feature_sheet_2, train_feature_sheet_1, feature0, features, feature1, feature_list\\n\\nwith open(\"feature_info.pkl\", \\'wb\\') as f:\\n    pickle.dump(feature_info, f)\\n\\ndir = \\'home-credit-credit-risk-model-stability\\'\\nfeature_info = pl.read_csv(\"feature_info.csv\")\\nfeature_info = feature_info.filter(pl.col(\"nan_ratio\") <= 0.7)\\nfeat_defs = pl.read_csv(os.path.join(dir0, \\'feature_definitions.csv\\'))\\nfeat_defs = feat_defs.filter(pl.col(\"Variable\").is_in(feature_info[\"feature\"]))\\ndel dir, feature_info, feat_defs\\n'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","# 保留feature_list中Variable列的值在train_feature_sheet_1中的行\n","train_feature_sheet_2 = train_feature_sheet_1[train_feature_sheet_1['feature'].isin(features1)]\n","feature_info = pl.DataFrame(train_feature_sheet_2.reset_index())\n","del train_feature_sheet_2, train_feature_sheet_1, feature0, features, feature1, feature_list\n","\n","with open(\"feature_info.pkl\", 'wb') as f:\n","    pickle.dump(feature_info, f)\n","\n","dir = 'home-credit-credit-risk-model-stability'\n","feature_info = pl.read_csv(\"feature_info.csv\")\n","feature_info = feature_info.filter(pl.col(\"nan_ratio\") <= 0.7)\n","feat_defs = pl.read_csv(os.path.join(dir0, 'feature_definitions.csv'))\n","feat_defs = feat_defs.filter(pl.col(\"Variable\").is_in(feature_info[\"feature\"]))\n","del dir, feature_info, feat_defs\n","'''"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.870498Z","iopub.status.busy":"2024-05-13T13:25:56.870012Z","iopub.status.idle":"2024-05-13T13:25:56.882397Z","shell.execute_reply":"2024-05-13T13:25:56.881077Z","shell.execute_reply.started":"2024-05-13T13:25:56.870456Z"},"trusted":true},"outputs":[],"source":["DATA_T = {\"Num\" : [\"Number\", \"number\", \"Amount\", \"amount\", \"limit\", \"Value\", \"value\", \"sum\", \"Sum\"], \n","            \"Factor\": [\"Flag\", \"flag\", \"Index\", \"index\", \"Indices\", \"indices\", \"Type\", \"type\",\n","                      \"Indicates\", \"indicates\", \"Status\", \"status\", \"Order\", \"Gender\", \"indicating\", \"reason\", \n","                      \"Reason\", \"Classification\", \"classification\", \"Name\", \"District\", \"Zipcode\", \"address\", \"language\", \n","                      \"Category\", \"category\", \"Role\", \"role\", \"Year\", \"year\"],\n","            \"Ratio\": [\"Rate\", \"rate\", \"Percentage\", \"percentage\"], \n","            \"Time\":  [\"Date\", \"date\"]}\n","\n","DATA_L = {\"Num\" : [\"Number\", \"number\", \"Amount\", \"amount\", \"DPD\", \"Days\" \"Average\", \"average\", \"limit\", \"Value\", \"value\", \n","                      \"Sum\", \"sum\"], \n","            \"Factor\": [\"Flag\", \"flag\", \"Index\", \"index\", \"Indices\", \"indices\", \"Type\", \"type\",\n","                      \"Indicates\", \"indicates\", \"Status\", \"status\", \"Order\", \"Gender\", \"indicating\", \"reason\", \n","                      \"Reason\", \"Classification\", \"classification\", \"Name\", \"District\", \"Zipcode\", \"address\", \"language\", \n","                      \"Category\", \"category\", \"Role\", \"role\", \"Year\", \"year\"],\n","            \"Ratio\": [\"Rate\", \"rate\", \"Percentage\", \"percentage\"],\n","            \"Time\":  [\"Date\", \"date\"]}"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T14:12:58.586330Z","iopub.status.busy":"2024-05-13T14:12:58.585792Z","iopub.status.idle":"2024-05-13T14:12:58.617667Z","shell.execute_reply":"2024-05-13T14:12:58.616308Z","shell.execute_reply.started":"2024-05-13T14:12:58.586293Z"},"trusted":true},"outputs":[],"source":["class Pipeline:\n","    \n","   \n","    @staticmethod\n","    def set_table_dtypes(df):\n","        # 读取特征定义文件\n","        feat_defs = pl.read_csv(os.path.join(ROOT, 'feature_definitions.csv'))\n","        # 处理特征描述列的数据类型\n","        feat_defs = feat_defs.with_columns(pl.col(\"Description\").str.split(by=\" \")\n","                                     .alias(\"Description\"))\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\", \"target\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64).alias(col))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n","            elif col[-1] in (\"M\"):\n","                df = df.with_columns(pl.col(col).cast(pl.String).alias(col))\n","            elif col[-1] in (\"T\", \"L\"):\n","                feat_df = feat_defs.filter(pl.col(\"Variable\") == col).select(\"Description\")\n","                if not feat_df.is_empty():\n","                    words = feat_df[0][\"Description\"].to_list()\n","                    for word in words:\n","                        if word in DATA_T[\"Num\"] or word in DATA_L[\"Num\"]:\n","                            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n","                        elif word in DATA_T[\"Ratio\"] or word in DATA_L[\"Ratio\"]:\n","                            df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n","                        elif word in DATA_T[\"Factor\"] or word in DATA_L[\"Factor\"]:\n","                            df = df.with_columns(pl.col(col).cast(pl.String).alias(col))\n","                        elif word in DATA_T[\"Time\"] or word in DATA_L[\"Time\"]:\n","                            df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n","        \n","        return df\n","\n","    @staticmethod\n","    def handle_dates(df, dir_path, base_file_name):\n","        \n","        date_decision = pl.read_parquet(os.path.join(dir_path, base_file_name))    \\\n","                               .select(pl.col(\"case_id\"), pl.col(\"date_decision\").cast(pl.Date))\n","        if \"date_decision\" in df.columns and \"target\" in df.columns:\n","            df.drop(\"MONTH\")\n","        for col in df.columns:\n","            \n","            if df[col].dtype == pl.Date and col != \"date_decision\":\n","                \n","                # Calculate duration difference in days\n","\n","                df = df.join(date_decision, on='case_id', how='left')\n","                diff_days_col = (pl.col(col) - pl.col(\"date_decision\")).cast(pl.Duration)\n","                df = df.with_columns(diff_days_col.dt.total_days().alias(f\"{col}_diff_daysP\"))   \n","                df = df.drop(\"date_decision\")\n","                \n","                df = df.with_columns(\n","                                month_col = pl.col(col).dt.month().cast(pl.UInt8).alias(f\"month_{col}\"),\n","                                weekday_col = pl.col(col).dt.weekday().cast(pl.UInt8).alias(f\"weekday_{col}\"),\n","                            )\n","                \n","                \n","        return df\n","    \n","          \n","    @staticmethod\n","    def filter_cols(df):\n","        \n","        for col in df.columns:\n","            if col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.drop(col)\n","        \n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                isnull = df[col].is_null().mean()\n","                if isnull > 0.7:\n","                    df = df.drop(col)\n","        \n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]) \\\n","                & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","        \n","        return df\n","        "]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.915839Z","iopub.status.busy":"2024-05-13T13:25:56.915442Z","iopub.status.idle":"2024-05-13T13:25:56.936254Z","shell.execute_reply":"2024-05-13T13:25:56.934931Z","shell.execute_reply.started":"2024-05-13T13:25:56.915808Z"},"trusted":true},"outputs":[],"source":["class Aggregator:\n","    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n","    def num_expr(df):\n","        cols = [col for col in df.columns if df[col].dtype == pl.datatypes.Float64]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        return expr_max +expr_last+expr_mean +expr_min \n","    \n","    def date_expr(df):\n","        cols = [col for col in df.columns if df[col].dtype == pl.Date]\n","        #expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        #expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        return  expr_first +expr_last\n","    \n","    def str_expr(df):\n","        cols = [col for col in df.columns if df[col].dtype == pl.String]\n","        # expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n","        return  expr_last +expr_count #+expr_max\n","    \n","    '''\n","    def other_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return  expr_max +expr_last\n","    '''\n","    \n","    def count_expr(df):\n","        cols = [col for col in df.columns if \"num_group\" in col]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return  expr_max +expr_last\n","    \n","    def get_exprs(df):\n","        exprs = Aggregator.num_expr(df) + \\\n","                Aggregator.date_expr(df) + \\\n","                Aggregator.str_expr(df) + \\\n","                Aggregator.count_expr(df)\n","\n","        return exprs"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T15:45:38.453403Z","iopub.status.busy":"2024-05-13T15:45:38.452909Z","iopub.status.idle":"2024-05-13T15:45:38.481121Z","shell.execute_reply":"2024-05-13T15:45:38.479748Z","shell.execute_reply.started":"2024-05-13T15:45:38.453363Z"},"trusted":true},"outputs":[],"source":["def read_file(path, depth=None):\n","    df = pl.read_parquet(path)\n","    df = df.pipe(Pipeline.set_table_dtypes)\n","    if depth in [1,2]:\n","        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n","        \n","    return df\n","\n","def read_files(regex_path, depth=None):\n","    chunks = []\n","    \n","    for path in glob(str(regex_path)):\n","        df = pl.read_parquet(path)\n","        df = df.pipe(Pipeline.set_table_dtypes)\n","        if depth in [1, 2]:\n","            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","        #df = df.fill_null(value=\"Temp_null_value\")\n","        chunks.append(df)\n","    \n","    df = pl.concat(chunks, how=\"vertical_relaxed\")\n","    #for col in df.columns:\n","    #    df = df.with_columns(pl.col(col).cast(pl.String))\n","    #    df = df.with_columns(pl.col(col).replace(df.filter(pl.col(col) == \"Temp_null_value\"),None))\n","    #df = df.pipe(Pipeline.set_table_dtypes)\n","    df = df.unique(subset=[\"case_id\"])\n","    \n","    return df\n","\n","def to_pandas(df_data, cat_cols=None):\n","    df_data = df_data.to_pandas()\n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    return df_data, cat_cols\n","\n","def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","    \n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        if str(col_type)==\"category\":\n","            continue\n","        \n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            continue\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    \n","    return df"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.965856Z","iopub.status.busy":"2024-05-13T13:25:56.965394Z","iopub.status.idle":"2024-05-13T13:25:56.983502Z","shell.execute_reply":"2024-05-13T13:25:56.982312Z","shell.execute_reply.started":"2024-05-13T13:25:56.965814Z"},"trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["os.path.exists(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:25:56.986045Z","iopub.status.busy":"2024-05-13T13:25:56.985682Z","iopub.status.idle":"2024-05-13T13:25:56.996341Z","shell.execute_reply":"2024-05-13T13:25:56.995094Z","shell.execute_reply.started":"2024-05-13T13:25:56.986016Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/home-credit-credit-risk-model-stability/csv_files/train\n"]}],"source":["ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n","TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n","TRAIN_CSV_DIR   = ROOT / \"csv_files\" / \"train\"\n","TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n","print(TRAIN_CSV_DIR)\n"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T16:00:26.851996Z","iopub.status.busy":"2024-05-13T16:00:26.851533Z","iopub.status.idle":"2024-05-13T16:04:32.057220Z","shell.execute_reply":"2024-05-13T16:04:32.055883Z","shell.execute_reply.started":"2024-05-13T16:00:26.851960Z"},"trusted":true},"outputs":[],"source":["data_store = {\n","    \"df_base\": [read_file(TRAIN_DIR / \"train_base.parquet\")],\n","    \"depth_0\": [\n","        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n","        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n","    ]\n","}\n","\n","with open('temporary_data_store.pkl', 'wb') as f:\n","    pickle.dump(data_store, f)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:29:55.334035Z","iopub.status.busy":"2024-05-13T13:29:55.333595Z","iopub.status.idle":"2024-05-13T13:29:55.385949Z","shell.execute_reply":"2024-05-13T13:29:55.384494Z","shell.execute_reply.started":"2024-05-13T13:29:55.333989Z"},"trusted":true},"outputs":[],"source":["'''\n","WOE-Encoder:\n","@Writer: BertrandBrelier\n","@Copy right: MIT Liscence\n","@Link: https://github.com/BertrandBrelier/woe/blob/main/woe.py\n","'''\n","\n","# DataFrame here is based on pandas rather than polars\n","class WoeConversion():\n","    def __init__(self, binarytarget, features, nbins=10):\n","        self.target = binarytarget\n","        self.features = features\n","        self.continuousvariables = []\n","        self.categoricalvariables = []\n","        self.categoricalmodel = None\n","        self.continuousmodel = None\n","        self.nbins = nbins\n","    def fit(self,df):\n","        #find continuous variables:\n","        traindf = df.copy(deep=True)\n","        try:\n","            traindf[self.target] = traindf[self.target].astype('int')\n","        except:\n","            print(\"ERROR : target variable must be a binary integer column with no missing values\")\n","            return \n","        for feat in self.features:\n","            if traindf[feat].dtypes == 'O':\n","                traindf[feat] = traindf[feat].astype(str)\n","                self.categoricalvariables.append(feat)\n","            else:\n","                try:\n","                    traindf[feat] = traindf[feat].astype(float)\n","                    self.continuousvariables.append(feat)\n","                except:\n","                    self.categoricalvariables.append(feat)\n","        self.categoricalmodel = ConvertCategoricalFeatures(binarytarget=self.target,CategoricalFeatures=self.categoricalvariables)\n","        self.categoricalmodel.fit(traindf)\n","        self.continuousmodel = ConvertContinuousFeatures(binarytarget=self.target,ContinuousFeatures=self.continuousvariables, NBins = self.nbins)\n","        self.continuousmodel.fit(traindf)\n","    def transform(self,testdf):\n","        tmpdf = testdf.copy(deep=True)\n","        tmpdf = self.categoricalmodel.transform(tmpdf)\n","        tmpdf = self.continuousmodel.transform(tmpdf)\n","        return tmpdf\n","        \n","class ConvertCategoricalFeatures():\n","    #Class to convert categorical features to WOE for binary classification problem (target = 0 or 1)\n","    def __init__(self,binarytarget,CategoricalFeatures):\n","        self.target = binarytarget\n","        self.Model = {}\n","        self.Features = CategoricalFeatures\n","    def fit(self,traindf):\n","        NPositive=traindf[traindf[self.target]==1].shape[0]\n","        NNegative=traindf[traindf[self.target]==0].shape[0]\n","        for feature in self.Features:\n","            tmptraindf = traindf[[feature,self.target]].copy(deep=True)\n","            tmptraindf[self.target] = tmptraindf[self.target].astype(int)\n","            tmptraindf[feature] = tmptraindf[feature].astype(str)\n","            results = tmptraindf[[feature,self.target]].fillna(\"None\").groupby([feature]).agg(['sum','count'])\n","            results = results.reset_index()\n","            results.columns=[feature,\"Positive\",\"Count\"]\n","            results[\"Negative\"]=results[\"Count\"]-results[\"Positive\"]\n","            results[\"CountPositive\"] = results[\"Positive\"]\n","            #Replace 0 with 1 to avoid infinite log                                                                                                          \n","            results.loc[results.Negative == 0, 'Negative'] = 1\n","            results.loc[results.Positive == 0, 'Positive'] = 1\n","            #Distribution Positive (Good)                                                                                                                    \n","            results[\"DG\"]=results[\"Positive\"]*1./NPositive\n","            #Distribution Negative (Bad)                                                                                                                     \n","            results[\"DB\"]=results[\"Negative\"]*1./NNegative\n","            #WOE                                                                                                                                             \n","            results[\"WOE\"]=np.log(results[\"DG\"]/results[\"DB\"])\n","            results.loc[results.Count <= 10, 'WOE'] = 0\n","            results.loc[results.CountPositive <= 1, 'WOE'] = results[\"WOE\"].min()\n","            results.loc[results.Count <= 10, 'WOE'] = 0\n","            results = results[[feature,'WOE']]\n","            self.Model[feature] = dict(zip(results[feature], results.WOE))\n","    def train(self,traindf):\n","        self.fit(traindf)\n","    def transform(self,testdf):\n","        #In case new values are found, needs to impute 0 for WOE\n","        for feature in self.Features:\n","            testdf[feature] = testdf[feature].astype(str)\n","            testdf = testdf.fillna({feature: \"None\"})\n","            ListofValues = list(set(testdf[feature].values))\n","            for omega in ListofValues:\n","                if omega not in self.Model[feature]:\n","                    self.Model[feature][omega]=0.\n","        return testdf.replace(self.Model)\n","\n","    \n","class ConvertContinuousFeatures():\n","    #Class to convert continuous features to WOE for binary classification problem (target = 0 or 1)\n","    def __init__(self,binarytarget,ContinuousFeatures,NBins):\n","        self.target = binarytarget\n","        self.Model = {}\n","        self.Features = ContinuousFeatures\n","        self.NBins = NBins\n","        self.BinModel = {}\n","    def train(self,traindf):\n","        self.fit(traindf)\n","    def fit(self,traindf):\n","        NPositive=traindf[traindf[self.target]==1].shape[0]\n","        NNegative=traindf[traindf[self.target]==0].shape[0]\n","        for feature in self.Features:\n","            tmpdf = traindf[[feature,self.target]].copy(deep=True)\n","            List = sorted(list(filter(lambda x:not np.isnan(x) ,tmpdf[feature].values)))\n","            Len = len(List)\n","            BinsLim = [-np.inf]\n","            for omega in range(1,self.NBins):\n","                Value = List[int(omega * Len / self.NBins)]\n","                if Value not in BinsLim:\n","                    BinsLim.append( Value )\n","            BinsLim.append(np.inf)\n","            self.BinModel[feature] = BinsLim\n","            tmpdf[\"bin\"] = pd.cut(tmpdf[feature], self.BinModel[feature], labels=range(1,len(self.BinModel[feature])))\n","            tmpdf[\"bin\"] = tmpdf[\"bin\"].cat.add_categories([-1])\n","            tmpdf[\"bin\"] = tmpdf[\"bin\"].fillna(-1) \n","            results = tmpdf[[\"bin\",self.target]].groupby([\"bin\"]).agg(['sum','count'])\n","            results = results.reset_index()\n","            results.columns=[\"bin\",\"Positive\",\"Count\"]\n","            results[\"Negative\"]=results[\"Count\"]-results[\"Positive\"]\n","            results[\"CountPositive\"] = results[\"Positive\"]\n","            #Replace 0 with 1 to avoid infinite log                                                                                                          \n","            results.loc[results.Negative == 0, 'Negative'] = 1\n","            results.loc[results.Positive == 0, 'Positive'] = 1\n","            #Distribution Positive (Good)                                                                                                                    \n","            results[\"DG\"]=results[\"Positive\"]*1./NPositive\n","            #Distribution Negative (Bad)                                                                                                                     \n","            results[\"DB\"]=results[\"Negative\"]*1./NNegative\n","            #WOE                                                                                                                                             \n","            results[\"WOE\"]=np.log(results[\"DG\"]/results[\"DB\"])\n","            results.loc[results.Count <= 10, 'WOE'] = 0\n","            results.loc[results.CountPositive <= 1, 'WOE'] = results[\"WOE\"].min()\n","            results.loc[results.Count <= 10, 'WOE'] = 0\n","            results = results[[\"bin\",'WOE']]\n","            self.Model[feature] = dict(zip(results[\"bin\"], results.WOE))\n","    def transform(self,testdf):\n","        tmpdf = testdf.copy(deep=True)\n","        for feature in self.Features:\n","            tmpdf[feature] = pd.cut(tmpdf[feature], self.BinModel[feature], labels=range(1,len(self.BinModel[feature])))\n","            tmpdf[feature] = tmpdf[feature].cat.add_categories([-1])\n","            tmpdf[feature] = tmpdf[feature].fillna(-1) \n","        return tmpdf.replace(self.Model)\n","        #return tmpdf.dtypes"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:29:55.388937Z","iopub.status.busy":"2024-05-13T13:29:55.388465Z","iopub.status.idle":"2024-05-13T13:29:56.470256Z","shell.execute_reply":"2024-05-13T13:29:56.468706Z","shell.execute_reply.started":"2024-05-13T13:29:55.388897Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class FeatureEngineering(object):\n","\n","    def __init__(self, path_dir, based_file_name):\n","        self.based_file_name = based_file_name\n","        self.dir = path_dir\n","    \n","    def balance_dataset(self, X_features_str=\"case_id\", y_features_str=\"target\", \n","                                under_ratio=0.5, over_ratio=0.5, batch_size=1000, \n","                                strategy = 'ENN', output=False):\n","        '''\n","        过采样与欠采样解决标签不均匀的问题\n","        '''\n","        df = pd.read_parquet(os.path.join(self.dir, self.based_file_name))\n","        X = np.array(df[X_features_str]).reshape(-1, 1)\n","        y = df[y_features_str]\n","        del df\n","        \n","        \n","        # 计算目标类别分布\n","        class_distribution = Counter(y)\n","        majority_class = max(class_distribution, key=class_distribution.get)\n","        minority_class = min(class_distribution, key=class_distribution.get)\n","        \n","        \n","        # 分批次处理数据\n","        X_resampled_batches = []\n","        y_resampled_batches = []\n","        for i in range(0, len(X), batch_size):\n","            X_batch = X[i:i+batch_size]\n","            y_batch = y[i:i+batch_size]\n","            \n","            # 计算欠采样和过采样的数量\n","            under_sample_size_batch = min(int(class_distribution[majority_class] * under_ratio)\n","                                          , len(X_batch))\n","            over_sample_size_batch = min(int(class_distribution[minority_class] * over_ratio)\n","                                          , len(X_batch))\n","            \n","\n","            if strategy == 'SMOTEENN':\n","                smoteenn = SMOTEENN(sampling_strategy={majority_class: under_sample_size_batch, \n","                                                   minority_class: over_sample_size_batch})\n","            elif strategy == 'ENN':\n","                smoteenn = EditedNearestNeighbours(sampling_strategy='auto', n_neighbors = 125)\n","            X_resampled_batch, y_resampled_batch = smoteenn.fit_resample(X_batch, y_batch)\n","            \n","            X_resampled_batches.append(X_resampled_batch)\n","            y_resampled_batches.append(y_resampled_batch)\n","            del X_resampled_batch, y_resampled_batch, X_batch, y_batch\n","        \n","        # 合并批次\n","        self.X_resampled = np.concatenate(X_resampled_batches)\n","        self.y_resampled = np.concatenate(y_resampled_batches)\n","        del X_resampled_batches, y_resampled_batches\n","        \n","        \n","        # 导出csv\n","        if output:\n","            df_resample = pd.DataFrame({X_features_str: self.X_resampled.flatten(), y_features_str: self.y_resampled})\n","            df_resample.to_csv(\"train_base_resample.csv\", index=False)\n","        else:\n","            return self.X_resampled, self.y_resampled\n","    \n","    \n","    def encoder(self, df):\n","        '''\n","        特征编码器\n","        '''\n","        cols_woe = []\n","        for col in df.columns:\n","            if col in [\"case_id\"]:\n","                pass\n","            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"max_num_group1\", \\\n","                         \"last_num_group1\", \"max_num_group2\", \"last_num_group2\"]:\n","                df.drop(pl.col(col))\n","            elif col in [\"date_decision\"]:\n","                pass\n","            elif df[col].dtype in [pl.Date]:\n","                         \n","                df = df.with_columns(\n","                                month_col = pl.col(col).dt.month().alias(f\"month_{col}\"),\n","                                weekday_col = pl.col(col).dt.weekday().alias(f\"weekday_{col}\"),\n","                                month_col_str = pl.col(f\"month_{col}\").dt.month().cast(pl.Utf8).alias(f\"weekday_{col}\"), \n","                                weekday_col_str  = pl.col(f\"weekday_{col}\").dt.weekday().cast(pl.Utf8).alias(f\"weekday_{col}\")\n","                            )\n","                df.drop(col)\n","                cols_woe.append(f\"month_{col}\", f\"weekday_{col}\")\n","                         \n","            elif df[col].dtype in [pl.datatypes.Int64, pl.datatypes.Int32, pl.datatypes.Float64, pl.datatypes.String]:\n","                #Encoding\n","                cols_woe.append(col)\n","                         \n","        woemodel = WoeConversion(binarytarget='target', features=cols_woe)\n","        woemodel.fit(df)\n","        df = woemodel.transform(df)\n","        \n","        with open(\"encoder.pkl\", \"wb\") as f:\n","            pickle.dump(woemodel, f) #需要时直接调用模型\n","                         \n","        return df  \n","        \n","        \n","        \n","    def df_data_compaction(self, df, base_case_ids):\n","                \n","        if not isinstance(base_case_ids, list):\n","            base_case_ids = list(base_case_ids.reshape((-1,)))\n","        df = df.filter(\n","                pl.col(\"case_id\").is_in(base_case_ids))\n","        df = df.pipe(Pipeline.handle_dates, self.dir, self.based_file_name) #传参可能有bug\n","        # df = df.pipe(Pipeline.filter_cols)\n","        return df \n","    \n","    \n","    def df_concatenates(self, data_store_dict):\n","        \n","        # 同一特征是否有重复合并之嫌\n","        feature_processed = []\n","        X_train = data_store_dict[\"df_base\"][0]\n","        for key, values in data_store_dict.items():\n","            print(\"Files is being concatenated: \", key)\n","            for value in values:\n","                # print(\"Type of value is \", type(value))\n","                feature_processing = []\n","                for column_name in value.columns:       \n","                    # justification\n","                    if column_name in feature_processed and not column_name in \\\n","                    [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\", \"max_num_group1\", \"last_num_group1\", \\\n","                     \"max_num_group2\", \"last_num_group2\", \"target\"]:\n","                        \n","                        X_train = X_train.join(value.select(pl.col(\"case_id\"), pl.col(column_name)), \n","                                                 on=\"case_id\", how=\"outer_coalesce\")\n","\n","                        X_train = X_train.with_columns(\n","                                    pl.when(pl.col(column_name).is_not_null()).then(pl.col(column_name)) \\\n","                                      .otherwise(pl.col(f\"{column_name}_right\")).alias(column_name) \n","                            ).drop(f\"{column_name}_right\")\n","                    elif column_name in \\\n","                    [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"max_num_group1\", \"last_num_group1\", \\\n","                     \"max_num_group2\", \"last_num_group2\"]:\n","                        pass\n","                    elif column_name == \"case_id\":\n","                        feature_processing.append(column_name)\n","                    elif column_name == \"target\":\n","                        pass\n","                    else: \n","                        feature_processing.append(column_name)\n","                        feature_processed.append(column_name)\n","                 \n","                #按照case_id左连接\n","                X_train = X_train.join(value.select(feature_processing), on='case_id', how=\"outer_coalesce\")\n","                del feature_processing\n","        del feature_processed\n","        gc.collect()\n","        \n","        return X_train                          \n","    \n","          \n","        \n","    def main(self):\n","    \n","    \n","        df_base = read_file(os.path.join(self.dir, self.based_file_name))\n","        case_ids, _ = self.balance_dataset()\n","        case_ids = list(case_ids.reshape((-1,)))\n","        del _\n","        \n","        \n","        with open('temporary_data_store.pkl', 'rb') as f:\n","            data_dict = {}\n","            while True:\n","                try:\n","                    data_loaded = pickle.load(f)\n","                    for key, value in data_loaded.items():\n","                        print(\"Processing files: \", key)\n","                        if key == 'df_base':\n","                            \n","                            \n","                            #重采样减少数据量\n","                            value = value[0]\n","                            df_base = df_base.filter(df_base['case_id'].is_in(case_ids)) \n","                            df_base = df_base.with_columns(\n","                                month_decision = pl.col(\"date_decision\").dt.month().alias(\"month_decision\"),\n","                                weekday_decision = pl.col(\"date_decision\").dt.weekday().alias(\"weekday_decision\"),\n","                            )\n","                            data_dict.update({key: [df_base]})\n","                            target = df_base[\"target\"]\n","                            del df_base\n","                            \n","                        elif key == 'depth_0' or 'depth_1' or 'depth_2':\n","                            #并行化处理函数\n","\n","                            from concurrent.futures import ThreadPoolExecutor\n","                            # 创建一个临时字典来存储处理后的 DataFrame\n","                            temp_value = {}\n","\n","                            with ThreadPoolExecutor(max_workers=3) as executor:\n","                                # 对 depth_* 中的每个 DataFrame 进行并行处理\n","                                futures = {i: executor.submit(self.df_data_compaction, df, case_ids) for i, df in enumerate(value)}\n","\n","                                # 等待所有任务完成\n","                                for i, future in futures.items():\n","                                    # 获取任务的结果\n","                                    filtered_df = future.result()\n","                                    # 将处理后的 DataFrame 存储到临时字典中\n","                                    temp_value[i] = filtered_df\n","                                \n","                            data_dict.update({key: list(temp_value.values())})\n","                        else:\n","                            data_dict.update({key: value})\n","                        print(\"Files processed: \", key)\n","                        \n","                        \n","                except EOFError:\n","                    # 到达文件末尾\n","                    break\n","        \n","        #合并\n","        train_data = self.df_concatenates(data_dict)\n","        #del data_dict\n","        #初步特征选择\n","        train_data = train_data.pipe(Pipeline.filter_cols)\n","        train_data = pd.DataFrame(train_data, columns = train_data.columns)\n","        #编码\n","        train_data = self.encoder(train_data)\n","\n","        # 将数据重新保存到 pkl 文件中\n","        with open('temporary_data_store.pkl', 'wb') as f:\n","            pickle.dump(data_dict, f)\n","      "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:32:38.300326Z","iopub.status.busy":"2024-05-13T13:32:38.299831Z","iopub.status.idle":"2024-05-13T13:32:38.368089Z","shell.execute_reply":"2024-05-13T13:32:38.366563Z","shell.execute_reply.started":"2024-05-13T13:32:38.300288Z"},"trusted":true},"outputs":[],"source":["class FeatureEngineering(object):\n","\n","    def __init__(self, path_dir, based_file_name):\n","        self.based_file_name = based_file_name\n","        self.dir = path_dir\n","    \n","    def balance_dataset(self, X_features_str=\"case_id\", y_features_str=\"target\", \n","                                under_ratio=0.5, over_ratio=0.5, batch_size=1000, \n","                                strategy = 'ENN', output=False):\n","        '''\n","        过采样与欠采样解决标签不均匀的问题\n","        '''\n","        df = pd.read_parquet(os.path.join(self.dir, self.based_file_name))\n","        X = np.array(df[X_features_str]).reshape(-1, 1)\n","        y = df[y_features_str]\n","        del df\n","        \n","        \n","        # 计算目标类别分布\n","        class_distribution = Counter(y)\n","        majority_class = max(class_distribution, key=class_distribution.get)\n","        minority_class = min(class_distribution, key=class_distribution.get)\n","        \n","        \n","        # 分批次处理数据\n","        X_resampled_batches = []\n","        y_resampled_batches = []\n","        for i in range(0, len(X), batch_size):\n","            X_batch = X[i:i+batch_size]\n","            y_batch = y[i:i+batch_size]\n","            \n","            # 计算欠采样和过采样的数量\n","            under_sample_size_batch = min(int(class_distribution[majority_class] * under_ratio)\n","                                          , len(X_batch))\n","            over_sample_size_batch = min(int(class_distribution[minority_class] * over_ratio)\n","                                          , len(X_batch))\n","            \n","\n","            if strategy == 'SMOTEENN':\n","                smoteenn = SMOTEENN(sampling_strategy={majority_class: under_sample_size_batch, \n","                                                   minority_class: over_sample_size_batch})\n","            elif strategy == 'ENN':\n","                smoteenn = EditedNearestNeighbours(sampling_strategy='auto', n_neighbors = 125)\n","            X_resampled_batch, y_resampled_batch = smoteenn.fit_resample(X_batch, y_batch)\n","            \n","            X_resampled_batches.append(X_resampled_batch)\n","            y_resampled_batches.append(y_resampled_batch)\n","            del X_resampled_batch, y_resampled_batch, X_batch, y_batch\n","        \n","        # 合并批次\n","        self.X_resampled = np.concatenate(X_resampled_batches)\n","        self.y_resampled = np.concatenate(y_resampled_batches)\n","        del X_resampled_batches, y_resampled_batches\n","        \n","        \n","        # 导出csv\n","        if output:\n","            df_resample = pd.DataFrame({X_features_str: self.X_resampled.flatten(), y_features_str: self.y_resampled})\n","            df_resample.to_csv(\"train_base_resample.csv\", index=False)\n","        else:\n","            return self.X_resampled, self.y_resampled\n","    \n","    \n","    def encoder(self, df):\n","        cols_woe = []\n","        polars_int = [pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Int64, pl.Int32, pl.Int8, pl.Int16]\n","        polars_str = [pl.String, str, object, pl.Boolean]\n","        polars_flt = [pl.Float32, pl.Float64]\n","        \n","        for col in df.columns:\n","            if col == \"case_id\":\n","                continue  # 跳过特定列 \"case_id\"\n","            elif col == \"target\":\n","                df = df.with_columns(pl.col(col).cast(pl.Int32))  # 将 \"target\" 列转换为 Int32 类型\n","            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"MONTH\", \"date_decision\"]:\n","                df = df.drop(col)  # 删除特定列\n","            elif df[col].dtype in polars_int or df[col].dtype in polars_flt:\n","                # 填充数值类型列的空值为平均值\n","                cols_woe.append(col)\n","                df = df.with_columns(pl.col(col).fill_null(pl.col(col).mean()).alias(col))\n","            elif df[col].dtype in polars_str:\n","                # 填充字符串类型列的空值为 \"NA\"\n","                cols_woe.append(col)\n","                df = df.with_columns(pl.col(col).fill_null(value=\"NA\").alias(col))\n","\n","        woemodel = WoeConversion(binarytarget='target', features=cols_woe)\n","        tuples_list = zip(df.columns, df.dtypes)\n","        dtype_dict = {k: v for k, v in tuples_list}\n","        \n","        # 将 pl.DataType 映射为 python datatype\n","        dtype_dict = {k: (int if v in polars_int else v) for k, v in dtype_dict.items()}\n","        dtype_dict = {k: (str if v in polars_str else v) for k, v in dtype_dict.items()}\n","        dtype_dict = {k: (float if v in polars_flt else v) for k, v in dtype_dict.items()}\n","        \n","        #这里要先处理缺失值\n","        df_pd = pd.DataFrame(df, columns=df.columns).astype(dtype_dict)\n","        woemodel.fit(df_pd)\n","        df_pd = woemodel.transform(df_pd)\n","        with open(\"encoder.pkl\", \"wb\") as f:\n","            pickle.dump(woemodel, f) #需要时直接调用模型\n","                         \n","        return df_pd  \n","        \n","        \n","        \n","    def df_data_compaction(self, df, base_case_ids):\n","                \n","        if not isinstance(base_case_ids, list):\n","            base_case_ids = list(base_case_ids.reshape((-1,)))\n","        df = df.filter(\n","                pl.col(\"case_id\").is_in(base_case_ids))\n","        df = df.pipe(Pipeline.handle_dates, self.dir, self.based_file_name) #传参可能有bug\n","        # df = df.pipe(Pipeline.filter_cols)\n","        return df \n","    \n","    \n","    def df_concatenates(self, data_store_dict):\n","        \n","        # 同一特征是否有重复合并之嫌\n","        feature_processed = []\n","        X_train = data_store_dict[\"df_base\"][0]\n","        for key, values in data_store_dict.items():\n","            for value in values:\n","                feature_processing = []\n","                for column_name in value.columns:       \n","                    # justification\n","                    if column_name in feature_processed and not column_name in data_store_dict[\"df_base\"][0].columns:\n","                        '''\n","                                                                               [\"case_id\", \"target\", \"WEEK_NUM\", \\\n","                                                                                \"num_group1\", \"num_group2\", \"max_num_group1\", \\\n","                                                                                \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n","                                                                                \"date_decision\", \"weekday_decision\", \"month_decision\", \\\n","                                                                                \"MONTH\"]:\n","                       '''\n","                        \n","                        new_column = X_train.join(value.select(pl.col(\"case_id\"), pl.col(column_name)), \n","                                                 on=\"case_id\", how=\"outer_coalesce\")\n","\n","                        new_column = new_column.with_columns(\n","                                    pl.when(pl.col(column_name).is_not_null()).then(pl.col(column_name)) \\\n","                                      .otherwise(pl.col(f\"{column_name}_right\")).alias(column_name) \n","                            ).drop(f\"{column_name}_right\")\n","                    elif column_name == \"case_id\":\n","                        feature_processing.append(column_name)\n","                    elif column_name == \"target\":\n","                        pass\n","                    elif column_name in data_store_dict[\"df_base\"][0].columns:\n","                        '''\n","                                        [\"WEEK_NUM\", \"num_group1\", \"num_group2\",\\\n","                                         \"max_num_group1\", \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n","                                         \"date_decision\", \"weekday_decision\", \"month_decision\", \"MONTH\"]:\n","                        '''\n","                        pass\n","                    elif value[column_name].dtype == pl.Date:\n","                        pass\n","                    else: \n","                        feature_processing.append(column_name)\n","                        feature_processed.append(column_name)\n","                 \n","                #按照case_id左连接\n","                X_train = X_train.join(value.select(feature_processing), on='case_id', how=\"outer_coalesce\")\n","                del feature_processing\n","        del feature_processed\n","        gc.collect()\n","        \n","        return X_train                          \n","    \n","          \n","        \n","    def main(self):\n","    \n","    \n","        df_base = read_file(os.path.join(self.dir, self.based_file_name))\n","        case_ids, _ = self.balance_dataset()\n","        case_ids = list(case_ids.reshape((-1,)))\n","        del _\n","        \n","        \n","        with open('temporary_data_store.pkl', 'rb') as f:\n","            data_dict = {}\n","            while True:\n","                try:\n","                    data_loaded = pickle.load(f)\n","                    for key, value in data_loaded.items():\n","                        print(\"Processing files: \", key)\n","                        if key == 'df_base':\n","                            \n","                            \n","                            #重采样减少数据量\n","                            value = value[0]\n","                            df_base = df_base.filter(df_base['case_id'].is_in(case_ids)) \n","                            df_base = df_base.with_columns(\n","                                month_decision = pl.col(\"date_decision\").dt.month().alias(\"month_decision\"),\n","                                weekday_decision = pl.col(\"date_decision\").dt.weekday().alias(\"weekday_decision\"),\n","                            )\n","                            data_dict.update({key: [df_base]})\n","                            target = df_base[\"target\"]\n","                            del df_base\n","                            \n","                        elif key == 'depth_0' or 'depth_1' or 'depth_2':\n","                            #并行化处理函数\n","\n","                            from concurrent.futures import ThreadPoolExecutor\n","                            # 创建一个临时字典来存储处理后的 DataFrame\n","                            temp_value = {}\n","\n","                            with ThreadPoolExecutor(max_workers=3) as executor:\n","                                # 对 depth_* 中的每个 DataFrame 进行并行处理\n","                                futures = {i: executor.submit(self.df_data_compaction, df, case_ids) for i, df in enumerate(value)}\n","\n","                                # 等待所有任务完成\n","                                for i, future in futures.items():\n","                                    # 获取任务的结果\n","                                    filtered_df = future.result()\n","                                    # 将处理后的 DataFrame 存储到临时字典中\n","                                    temp_value[i] = filtered_df\n","                                    del filtered_df\n","\n","                            data_dict.update({key: list(temp_value.values())})\n","                        else:\n","                            data_dict.update({key: value})\n","                        print(\"Files processed: \", key)\n","                        \n","                        \n","                except EOFError:\n","                    # 到达文件末尾\n","                    break\n","        \n","        #合并\n","        train_data = self.df_concatenates(data_dict)\n","        del data_dict\n","        #初步特征选择\n","        train_data = train_data.pipe(Pipeline.filter_cols)\n","        #编码\n","        train_data = self.encoder(train_data)\n","\n","        # 将数据重新保存到 pkl 文件中\n","        with open('train_data_store.pkl', 'wb') as f:\n","            pickle.dump(train_data, f) "]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:38:44.594339Z","iopub.status.busy":"2024-05-13T13:38:44.593907Z","iopub.status.idle":"2024-05-13T13:38:44.638833Z","shell.execute_reply":"2024-05-13T13:38:44.637586Z","shell.execute_reply.started":"2024-05-13T13:38:44.594308Z"},"trusted":true},"outputs":[],"source":["class TestFeatureEngineering(object):\n","\n","    \n","    def __init__(self, path_dir, based_file_name):\n","        self.based_file_name = based_file_name\n","        self.dir = path_dir\n","        \n","        \n","    def df_data_compaction(self, df, base_case_ids):\n","                \n","        if not isinstance(base_case_ids, list):\n","            base_case_ids = list(base_case_ids.reshape((-1,)))\n","        df = df.filter(\n","                pl.col(\"case_id\").is_in(base_case_ids))\n","        df = df.pipe(Pipeline.handle_dates, self.dir, self.based_file_name) #传参可能有bug\n","        # df = df.pipe(Pipeline.filter_cols)\n","        return df \n","    \n","    \n","    def df_concatenates(self, data_store_dict):\n","        \n","        # 同一特征是否有重复合并之嫌\n","        feature_processed = []\n","        X_train = data_store_dict[\"df_base\"][0]\n","        for key, values in data_store_dict.items():\n","            for value in values:\n","                feature_processing = []\n","                for column_name in value.columns:       \n","                    # justification\n","                    if column_name in feature_processed and not column_name in data_store_dict[\"df_base\"][0].columns:\n","                        '''\n","                                                                               [\"case_id\", \"target\", \"WEEK_NUM\", \\\n","                                                                                \"num_group1\", \"num_group2\", \"max_num_group1\", \\\n","                                                                                \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n","                                                                                \"date_decision\", \"weekday_decision\", \"month_decision\", \\\n","                                                                                \"MONTH\"]:\n","                       '''\n","                        \n","                        new_column = X_train.join(value.select(pl.col(\"case_id\"), pl.col(column_name)), \n","                                                 on=\"case_id\", how=\"outer_coalesce\")\n","\n","                        new_column = new_column.with_columns(\n","                                    pl.when(pl.col(column_name).is_not_null()).then(pl.col(column_name)) \\\n","                                      .otherwise(pl.col(f\"{column_name}_right\")).alias(column_name) \n","                            ).drop(f\"{column_name}_right\")\n","                    elif column_name == \"case_id\":\n","                        feature_processing.append(column_name)\n","                    elif column_name == \"target\":\n","                        pass\n","                    elif column_name in data_store_dict[\"df_base\"][0].columns:\n","                        '''\n","                                        [\"WEEK_NUM\", \"num_group1\", \"num_group2\",\\\n","                                         \"max_num_group1\", \"last_num_group1\", \"max_num_group2\", \"last_num_group2\", \\\n","                                         \"date_decision\", \"weekday_decision\", \"month_decision\", \"MONTH\"]:\n","                        '''\n","                        pass\n","                    elif value[column_name].dtype == pl.Date:\n","                        pass\n","                    else: \n","                        feature_processing.append(column_name)\n","                        feature_processed.append(column_name)\n","                 \n","                #按照case_id左连接\n","                X_train = X_train.join(value.select(feature_processing), on='case_id', how=\"outer_coalesce\")\n","                del feature_processing\n","        del feature_processed\n","        gc.collect()\n","        \n","        return X_train\n","    \n","    \n","    def encoding_transform(self, df, encoder_fitted):\n","        cols_woe = []\n","        polars_int = [pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Int64, pl.Int32, pl.Int8, pl.Int16]\n","        polars_str = [pl.String, str, object, pl.Boolean]\n","        polars_flt = [pl.Float32, pl.Float64]\n","        \n","        for col in df.columns:\n","            if col == \"case_id\":\n","                continue  # 跳过特定列 \"case_id\"\n","            elif col == \"target\":\n","                df = df.with_columns(pl.col(col).cast(pl.Int32))  # 将 \"target\" 列转换为 Int32 类型\n","            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"MONTH\", \"date_decision\"]:\n","                df = df.drop(col)  # 删除特定列\n","            elif df[col].dtype in polars_int or df[col].dtype in polars_flt:\n","                # 填充数值类型列的空值为平均值\n","                cols_woe.append(col)\n","                df = df.with_columns(pl.col(col).fill_null(pl.col(col).mean()).alias(col))\n","            elif df[col].dtype in polars_str:\n","                # 填充字符串类型列的空值为 \"NA\"\n","                cols_woe.append(col)\n","                df = df.with_columns(pl.col(col).fill_null(value=\"NA\").alias(col))\n","\n","        tuples_list = zip(df.columns, df.dtypes)\n","        dtype_dict = {k: v for k, v in tuples_list}\n","        \n","        # 将 pl.DataType 映射为 python datatype\n","        dtype_dict = {k: (int if v in polars_int else v) for k, v in dtype_dict.items()}\n","        dtype_dict = {k: (str if v in polars_str else v) for k, v in dtype_dict.items()}\n","        dtype_dict = {k: (float if v in polars_flt else v) for k, v in dtype_dict.items()}\n","        \n","        #这里要先处理缺失值\n","        df_pd = pd.DataFrame(df, columns=df.columns).astype(dtype_dict)\n","        \n","        df_pd = encoder_fitted.transform(df_pd)\n","        return df_pd\n","    \n","\n","    def main(self, encoder_fitted):\n","    \n","    \n","        df_base = read_file(os.path.join(self.dir, self.based_file_name))\n","        case_ids = df_base.columns\n","        \n","        with open('temporary_data_store.pkl', 'rb') as f:\n","            data_dict = {}\n","            while True:\n","                try:\n","                    data_loaded = pickle.load(f)\n","                    for key, value in data_loaded.items():\n","                        print(\"Processing files: \", key)\n","                        if key == 'df_base':\n","                            \n","                            value = value[0]\n","                            df_base = df_base.filter(df_base['case_id'].is_in(case_ids)) \n","                            df_base = df_base.with_columns(\n","                                month_decision = pl.col(\"date_decision\").dt.month().alias(\"month_decision\"),\n","                                weekday_decision = pl.col(\"date_decision\").dt.weekday().alias(\"weekday_decision\"),\n","                            )\n","                            data_dict.update({key: [df_base]})\n","                            del df_base\n","                            \n","                        elif key == 'depth_0' or 'depth_1' or 'depth_2':\n","                            #并行化处理函数\n","\n","                            from concurrent.futures import ThreadPoolExecutor\n","                            # 创建一个临时字典来存储处理后的 DataFrame\n","                            temp_value = {}\n","\n","                            with ThreadPoolExecutor(max_workers=3) as executor:\n","                                # 对 depth_* 中的每个 DataFrame 进行并行处理\n","                                futures = {i: executor.submit(self.df_data_compaction, df, case_ids) for i, df in enumerate(value)}\n","\n","                                # 等待所有任务完成\n","                                for i, future in futures.items():\n","                                    # 获取任务的结果\n","                                    filtered_df = future.result()\n","                                    # 将处理后的 DataFrame 存储到临时字典中\n","                                    temp_value[i] = filtered_df\n","                                    del filtered_df\n","\n","                            data_dict.update({key: list(temp_value.values())})\n","                        else:\n","                            data_dict.update({key: value})\n","                        print(\"Files processed: \", key)\n","                        \n","                        \n","                except EOFError:\n","                    # 到达文件末尾\n","                    break\n","        \n","        #合并\n","        test_data = self.df_concatenates(data_dict)\n","        del data_dict\n","        #初步特征选择\n","        with open('train_data_store.pkl', 'rb') as f:\n","            train_data = pickle.load(train_data, f) \n","        test_data = test_data.filter(train_data.columns)\n","        del train_data\n","        #编码\n","        test_data = self.encoding_transform(test_data, encoder_fitted)\n","\n","        # 将数据重新保存到 pkl 文件中\n","        with open('test_data_store.pkl', 'wb') as f:\n","            pickle.dump(test_data, f) "]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T13:32:54.378032Z","iopub.status.busy":"2024-05-13T13:32:54.376584Z","iopub.status.idle":"2024-05-13T13:37:39.601482Z","shell.execute_reply":"2024-05-13T13:37:39.600077Z","shell.execute_reply.started":"2024-05-13T13:32:54.377987Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing files:  df_base\n","Files processed:  df_base\n","Processing files:  depth_0\n","Files processed:  depth_0\n","Processing files:  depth_1\n","Files processed:  depth_1\n","Processing files:  depth_2\n","Files processed:  depth_2\n"]}],"source":["#del data_store \n","gc.collect()\n","FeatureEngineering(path_dir= TRAIN_DIR, based_file_name=\"train_base.parquet\").main()"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T15:59:53.624632Z","iopub.status.busy":"2024-05-13T15:59:53.623844Z","iopub.status.idle":"2024-05-13T15:59:54.020245Z","shell.execute_reply":"2024-05-13T15:59:54.018506Z","shell.execute_reply.started":"2024-05-13T15:59:53.624594Z"},"trusted":true},"outputs":[{"ename":"ComputeError","evalue":"schema lengths differ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m data_store \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_base\u001b[39m\u001b[38;5;124m\"\u001b[39m: [read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_base.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      4\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_static_cb_0.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m         read_files(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_static_0_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m----> 8\u001b[0m         \u001b[43mTest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_applprev_1_*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      9\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_tax_registry_a_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     10\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_tax_registry_b_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_tax_registry_c_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     12\u001b[0m         read_files(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_a_1_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     13\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_b_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     14\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_other_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     15\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_person_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     16\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_deposit_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     17\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_debitcard_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     18\u001b[0m     ],\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     20\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_b_2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     21\u001b[0m         read_files(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_credit_bureau_a_2_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     22\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_applprev_2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     23\u001b[0m         read_file(TEST_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_person_2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporary_data_store.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     28\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(data_store, f)\n","Cell \u001b[0;32mIn[74], line 15\u001b[0m, in \u001b[0;36mTest.read_files\u001b[0;34m(regex_path, depth)\u001b[0m\n\u001b[1;32m     12\u001b[0m             df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(col)\u001b[38;5;241m.\u001b[39mfill_null(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemp_null_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(col))\n\u001b[1;32m     13\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvertical_relaxed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     17\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mcol(col)\u001b[38;5;241m.\u001b[39mcast(pl\u001b[38;5;241m.\u001b[39mString))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/polars/functions/eager.py:187\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(items, how, rechunk, parallel)\u001b[0m\n\u001b[1;32m    184\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_df(plr\u001b[38;5;241m.\u001b[39mconcat_df(elems))\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical_relaxed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_ldf(\n\u001b[0;32m--> 187\u001b[0m         \u001b[43mplr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_lf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melems\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mto_supertypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     )\u001b[38;5;241m.\u001b[39mcollect(no_optimization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagonal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    196\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_df(plr\u001b[38;5;241m.\u001b[39mconcat_df_diagonal(elems))\n","\u001b[0;31mComputeError\u001b[0m: schema lengths differ"]}],"source":["data_store = {\n","    \"df_base\": [read_file(TEST_DIR / \"test_base.parquet\")],\n","    \"depth_0\": [\n","        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n","        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        Test.read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n","        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n","    ]\n","}\n","\n","with open('temporary_data_store.pkl', 'wb') as f:\n","    pickle.dump(data_store, f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"encoder.pkl\", \"rb\") as f:\n","    woe_encoder = pickle.load(f) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-13T13:31:33.252312Z","iopub.status.idle":"2024-05-13T13:31:33.252940Z","shell.execute_reply":"2024-05-13T13:31:33.252657Z","shell.execute_reply.started":"2024-05-13T13:31:33.252630Z"},"trusted":true},"outputs":[],"source":["del data_store \n","gc.collect()\n","TestFeatureEngineering(path_dir= TEST_DIR, based_file_name=\"test_base.parquet\").main(woe_encoder)"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T15:59:50.399862Z","iopub.status.busy":"2024-05-13T15:59:50.399426Z","iopub.status.idle":"2024-05-13T15:59:50.422553Z","shell.execute_reply":"2024-05-13T15:59:50.421010Z","shell.execute_reply.started":"2024-05-13T15:59:50.399829Z"},"trusted":true},"outputs":[],"source":["class Test:  \n","\n","    def read_files(regex_path, depth=None):\n","        chunks = []\n","    \n","        for path in glob(str(regex_path)):\n","            df = pl.read_parquet(path)\n","            df = df.pipe(Pipeline.set_table_dtypes)\n","            if depth in [1, 2]:\n","                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","                for col in df.columns:\n","                    df = df.with_columns(pl.col(col).fill_null(value=\"0\").alias(col))\n","            chunks.append(df)\n","    \n","        df = pl.concat(chunks, how=\"vertical_relaxed\")\n","        for col in df.columns:\n","            df = df.with_columns(pl.col(col).cast(pl.String))\n","            df = df.with_columns(pl.col(col).replace(df.filter(pl.col(col) == \"Temp_null_value\"),None).alias(col))\n","        df = df.pipe(Pipeline.set_table_dtypes)\n","        df = df.unique(subset=[\"case_id\"])\n","    \n","        return df\n","            \n","            \n","    def encoder(df):\n","        cols_woe = []\n","        polars_int = [pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Int64, pl.Int32, pl.Int8, pl.Int16]\n","        polars_str = [pl.String, str, object, pl.Boolean]\n","        polars_flt = [pl.Float32, pl.Float64]\n","        \n","        for col in df.columns:\n","            if col == \"case_id\":\n","                continue  # 跳过特定列 \"case_id\"\n","            elif col == \"target\":\n","                df = df.with_columns(pl.col(col).cast(pl.Int32))  # 将 \"target\" 列转换为 Int32 类型\n","            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\", \"MONTH\", \"date_decision\"]:\n","                df = df.drop(col)  # 删除特定列\n","            elif df[col].dtype in polars_int or df[col].dtype in polars_flt:\n","                # 填充数值类型列的空值为平均值\n","                cols_woe.append(col)\n","                df = df.with_columns(pl.col(col).fill_null(pl.col(col).mean()).alias(col))\n","            elif df[col].dtype in polars_str:\n","                # 填充字符串类型列的空值为 \"NA\"\n","                cols_woe.append(col)\n","                df = df.with_columns(pl.col(col).fill_null(value=\"NA\").alias(col))\n","\n","        woemodel = WoeConversion(binarytarget='target', features=cols_woe)\n","        tuples_list = zip(df.columns, df.dtypes)\n","        dtype_dict = {k: v for k, v in tuples_list}\n","        \n","        # 将 pl.DataType 映射为 python datatype\n","        dtype_dict = {k: (int if v in polars_int else v) for k, v in dtype_dict.items()}\n","        dtype_dict = {k: (str if v in polars_str else v) for k, v in dtype_dict.items()}\n","        dtype_dict = {k: (float if v in polars_flt else v) for k, v in dtype_dict.items()}\n","        \n","        #这里要先处理缺失值\n","        df_pd = pd.DataFrame(df, columns=df.columns).astype(dtype_dict)\n","        \n","        woemodel.fit(df_pd)\n","        df_pd = woemodel.transform(df_pd)\n","        return df_pd"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
